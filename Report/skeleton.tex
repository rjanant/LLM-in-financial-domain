% MSc dissertation example file, February 2022
%
% Leave one of the documentclass lines uncommented to match your degree.
% You may remove the logo option if it causes problems.
% Do not change any other options.
% \documentclass[logo,msc,adi]{infthesis}     % Adv Design Inf
% \documentclass[logo,msc,ai]{infthesis}      % AI
% \documentclass[logo,msc,cogsci]{infthesis}  % Cognitive Sci
% \documentclass[logo,msc,cs]{infthesis}      % Computer Sci
% \documentclass[logo,msc,cyber]{infthesis}   % Cyber Sec
% \documentclass[logo,msc,datasci]{infthesis} % Data Sci
% \documentclass[logo,msc,di]{infthesis}      % Design Inf
% \documentclass[logo,msc,dsti]{infthesis}    % Data Sci TI
% \documentclass[logo,msc,inf]{infthesis}     % Informatics
\documentclass[logo,msc]{infthesis}           % degree unspecified, do not change except to add your degree
%%%%%%%%%%%%%%%%%%%%%%%%
% Understand any problems and seek approval before assuming it's ok to remove ugcheck.
\usepackage{msccheck}

% Include any packages you need below, but don't include any that change the page
% layout or style of the dissertation. By including the ugcheck package above,
% you should catch most accidental changes of page layout though.

\usepackage{microtype} % recommended, but you can remove if it causes problems

% MY PACKAGES 
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,fit,backgrounds}
%\usepackage[most]{tcolorbox} % Load tcolorbox with most libraries
\usepackage{tcolorbox}
\usepackage{amsmath} % For math symbols and align environment
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{array}
\usepackage{longtable} % Allows tables to span multiple pages if needed
\usepackage{listings} % Include the listings package for formatted code blocks
\usepackage{xcolor} % For colored text
\usepackage{geometry}



% ends here packages
\begin{document}
\begin{preliminary}

\title{Large language model number handling in the Finance Domain}

\author{Anant Raj}

\date{\today}

\abstract{
% This skeleton demonstrates how to use the \texttt{infthesis} style for
% MSc dissertations in the School of Informatics. It also emphasises the
% page limit and associated style restrictions for Informatics dissertations
% with course code \texttt{INFR11077}. If your degree has a different project
% course code, then it is likely to have different formatting rules.
% The file \texttt{skeleton.tex} generates this document and should be used as a
% starting point for your thesis. Replace this abstract text with a concise
% summary of your report.
This research project focuses on enhancing the performance of large language models (LLMs) in finance by improving their capabilities in numerical data handling and financial reasoning. By employing a mixture of financial datasets our approach encompasses specialized instruction tuning methods alongside the strategic use of specialised external computational tools for specific tasks. The project rigorously fine-tuned open-source LLMs like Llama2,Llama3 and evaluate their performance in comparison with commercial models such as GPT-4. The principal aim is to forge a robust framework for the processing and analysis of financial data, which minimizes human intervention while enhancing accuracy and efficiency, thus redefining the benchmark for AI applications in financial analysis.
}

\maketitle

\newenvironment{ethics}
   {\begin{frontenv}{Research Ethics Approval}{\LARGE}}
   {\end{frontenv}\newpage}

\begin{ethics}
% \textbf{Instructions:} \emph{Agree with your supervisor which
% statement you need to include. Then delete the statement that you are not using,
% and the instructions in italics.\\
% \textbf{Either complete and include this statement:}}\\ % DELETE THESE INSTRUCTIONS
% %
% % IF ETHICS APPROVAL WAS REQUIRED:
% This project obtained approval from the Informatics Research Ethics committee.\\
% Ethics application number: ???\\
% Date when approval was obtained: YYYY-MM-DD\\
% %
% \emph{[If the project required human participants, edit as appropriate, otherwise delete:]}\\ % DELETE THIS LINE
% The participants' information sheet and a consent form are included in the appendix.\\
%
% IF ETHICS APPROVAL WAS NOT REQUIRED:
% \textbf{\emph{Or include this statement:}}\\ % DELETE THIS LINE
This project was planned in accordance with the Informatics Research
Ethics policy. It did not involve any aspects that required approval
from the Informatics Research Ethics committee.

\standarddeclaration
\end{ethics}


\begin{acknowledgements}
% Any acknowledgements go here.

I am profoundly grateful to my supervisor, Alexandra Birch, whose expertise, understanding, and patience added substantially to my graduate experience. I deeply appreciate the countless hours she spent discussing experimental results with me and her insightful comments that continually inspired me throughout this journey. Her comprehensive guidance and unwavering support were instrumental in the successful completion of this project.

I extend my heartfelt thanks to Mateusz, whose encouragement was a beacon of light during times of struggle. His motivating words helped me navigate through the challenging moments of my research.

My sincere gratitude also goes out to my friends and family, whose unending support and love provided me with strength and encouragement. Their belief in my abilities sustained me throughout this academic endeavor.

This dissertation stands as a testament not only to my efforts but also to the valuable contributions of all those who supported me along the way. Thank you.

\end{acknowledgements}


\tableofcontents
\end{preliminary}


\chapter{Introduction}

The intersection of Large Language Models (LLMs) and the financial industry represents a frontier of considerable promise and complexity. As documented by recent advancements, LLMs such as ChatGPT and GPT-4 have showcased remarkable proficiencies across a spectrum of Natural Language Processing (NLP) tasks, driven by sophisticated training methodologies including reinforcement learning from human feedback (RLHF) and masked language model objectives \cite{li2023chatgpt}. However, recent advancements have highlighted their limitations, such as the inability to access current information, difficulties with arithmetic and mathematical computations, and a propensity to generate inaccurate responses due to hallucinations \cite{schick2023toolformer,li2024dawn}. Despite being trained on diverse datasets covering multiple genres and subjects, their effectiveness in specialized domains like finance still necessitates further scrutiny.

In the financial arena, LLMs have begun to play an indispensable role, aiding in tasks such as investment sentiment analysis, financial named entity recognition, question-answering systems, and stock market prediction, which assist financial analysts in navigating complex datasets and predictive models. Methods such as multi-field LLMs and instruction fine-tuned LLMs have been explored to enhance the efficacy of generated results in the financial context \cite{lee2024survey}. Nonetheless, the nuanced understanding and processing of financial data by LLMs — critical for applications ranging from market analysis to investment strategy formulation — remain at an early stage for empirical study and innovation.

This project aims to investigate various number-handling strategies to fine-tune a large language model using instruction data, with the goal of achieving optimal performance in finance-related downstream tasks, particularly those involving currency or numerical questions. A diverse set of financial datasets has been selected, encompassing tabular data, contextual information, relation extraction, and Conversational Finance Question Answering. These complex numerical reasoning datasets provide a robust foundation for generalizing across a wide range of financial tasks.

Initially, baseline performance was assessed on open-source models such as Llama2 and the newly launched Llama3 using these datasets. It is well known that large language models (LLMs) often struggle with numerical data, particularly in tasks requiring numerical reasoning, and our results were consistent with this observation. Several fine-tuning approaches were explored, starting with the Llama2-chat 7B model and subsequently extending experiments to the latest Llama3 8B model. Recognizing that a single prompting technique does not suffice for all scenarios, various techniques were experimented with, yielding notable results detailed in the experiment section.

In our study, we venture beyond the traditional confines of language model applications, drawing upon the groundbreaking work of Schick et al. (2023) who utilized external tools to elevate the performance of models handling numerical values over those processing textual data. This approach aligns with cutting-edge developments like LangChain agents and function calling, which empower models to leverage external capabilities effectively.

Our research work explores the potential of LLMs in processing and analyzing intricate financial datasets, addressing the current limitations in computational power and the scarcity of open-source models capable of handling extensive contextual inputs typical of financial data. Our study operates under the hypothesis that, with optimal resource allocation and fine-tuning, LLMs can significantly enhance their script generation capabilities, potentially surpassing existing benchmarks in numerical accuracy and analytical depth. To investigate this hypothesis, we conducted a two-phase pilot study. Initially, we utilized a compact, curated dataset sample with the \textsc{facebook/opt-1.3b} model, integrated with LangChain's Python agent (\textit{python\_repl}), which yielded promising results. Building on this success, we scaled up our experiment, employing an LLM to generate Python scripts for more extensive financial datasets, executed through Python's \textit{exec \footnote{https://docs.python.org/3/library/functions.html#exec}} function in a pipeline-like structure. While this expanded experiment did not achieve the same level of accuracy as the smaller, handcrafted sample, it produced intriguing results that highlight the critical role of model size and sophistication in code generation based on complex inputs. 

We posit that leveraging larger, more advanced language models with proper fine tuning and prompting could potentially bridge this performance gap. This research not only contributes to the growing body of knowledge at the intersection of natural language processing and financial analytics but also raises compelling questions about the scalability of LLMs in financial analysis, the efficacy of fine-tuning strategies, and the optimal balance between model size, computational efficiency, and analytical accuracy. As we navigate these challenges, our study lays a robust foundation for future investigations, potentially paving the way for significant advancements in how we approach and execute complex financial analyses using open source LLMs.

%Our experiments are not merely replications but expansions, tailored to explore uncharted territories in financial analytics. We innovated upon existing methodologies by implementing a sophisticated pipeline structure. Initially, financial datasets serve as input for code-generating large language models (LLMs), tasked with crafting precise Python scripts based on the nuanced demands of the financial context. These scripts are then executed by another model functioning within a Python REPL and or Python Exec environment, aimed at delivering the final outputs.

%Given the inherent challenges of computational limitations and the absence of open-source LLMs capable of processing extensive contextual inputs typical of financial datasets, we proceeded under the hypothesis that, with optimal resources and fine-tuning, our language model could refine its script generation to achieve unprecedented accuracy levels, potentially surpassing current benchmarks. Our pilot study, utilizing a compact dataset sample with the \textsc{facebook/opt-1.3b} model in conjunction with LangChain's Python agent, python\_repl, yielded promising results. These initial findings not only demonstrate the model's capability but also mark a pivotal first step towards scaling up and enhancing the precision and analytical prowess of LLMs when faced with larger and more complex data sets. Upon receiving promising result using the handcrafted small dataset sample we extended our experiment to use a llm to generate the python script on financial dataset and run the generated script thorugh python's exec function maintaining a pipeline like structure to generate the final results. Although, we couldn't achieve the same level of accuracy as we did with the smaller sample and handcrafted data, we got some pretty interesting results and we hypothesise that using larger language models even better results could be achieved since it mostly depends on the ability of language model on how well it can generate code based on the given input. Several interesting questions arises and we aim to answer some of them in this project and this reserach project sets a robust foundation for future research to build upon, aiming for significant advancements in the application of LLMs to financial tasks. 

% Schick et al. (2023) have conducted noteworthy research utilizing external tools to address the suboptimal performance of models with numerical values compared to textual information. Separately, recent advancements such as LangChain agents and function calling have been developed to enable models to call agents and use external tools. Our experiments draw inspiration from both these approaches, attempting to integrate their usage \cite{schick2023toolformerlanguagemodelsteach}. However, these techniques have not yet been extended to financial tasks or number handling. Our experiments extended these advancements by employing a pipeline structure: first, inputs from the financial datasets were fed into code generation LLMs to generate accurate Python scripts based on contextual input; these scripts were then passed on to another model to generate the final answer using Python REPL. Despite being constrained by computational limits and time constraints, particularly with no open-source LLMs capable of generating correct Python scripts from long context inputs typically found in financial datasets, we proceeded with the hypothesis that, given proper resources and tuning, a language model could generate precise scripts to achieve higher accuracy, surpassing the state of the art (SOTA). An experiment was conducted using a small dataset sample, wherein a small open-source model, 'facebook/opt-1.3b', utilized LangChain's Python agent, python\_repl, to generate promising results. These results serve as a valuable starting point for scalability and enhancing the precision and reasoning capability of LLMs with larger data samples.

% The preliminary material of your report should contain:
% \begin{itemize}
% \item
% The title page.
% \item
% An abstract page.
% \item
% Declaration of ethics and own work.
% \item
% Optionally an acknowledgements page.
% \item
% The table of contents.
% \end{itemize}

% As in this example \texttt{skeleton.tex}, the above material should be
% included between:
% \begin{verbatim}
% \begin{preliminary}
%     ...
% \end{preliminary}
% \end{verbatim}
% This style file uses roman numeral page numbers for the preliminary material.

% The main content of the dissertation, starting with the first chapter,
% starts with page~1. \emph{\textbf{The main content must not go beyond page~40.}}

% The report then contains a bibliography and any appendices, which may go beyond
% page~40. The appendices are only for any supporting material that's important to
% go on record. However, you cannot assume markers of dissertations will read them.

% You may not change the dissertation format (e.g., reduce the font size, change
% the margins, or reduce the line spacing from the default 1.5 spacing). Be
% careful if you copy-paste packages into your document preamble from elsewhere.
% Some \LaTeX{} packages, such as \texttt{fullpage} or \texttt{savetrees}, change
% the margins of your document. Do not include them!

% Over-length or incorrectly-formatted dissertations will not be accepted and you
% would have to modify your dissertation and resubmit. You cannot assume we will
% check your submission before the final deadline and if it requires resubmission
% after the deadline to conform to the page and style requirements you will be
% subject to the usual late penalties based on your final submission time.

% \section{Using Sections}

% Divide your chapters into sub-parts as appropriate.

% \section{Citations}

% Citations (such as \cite{P1} or \cite{P2}) can be generated using
% \texttt{BibTeX}. For more advanced usage, we recommend using the \texttt{natbib}
% package or the newer \texttt{biblatex} system.

% These examples use a numerical citation style. You may use any consistent
% reference style that you prefer, including ``(Author, Year)'' citations.

\chapter{Background}

% This chapter aims to remind readers of background knowledge of this project. Section 2.1 introduces the structure of transformers. Self-attention mechanism of the transformer is described in section 2.2. Section 2.3 discusses position embeddings in detail. Section 2.4 introduces the background knowledge of residual connections.

The field of natural language processing (NLP) has experienced a profound transformation in recent years, primarily propelled by the introduction of transformer-based models and the subsequent development of large language models (LLMs). This chapter presents an overview of the evolution of these models. Section 2.1 introduces the architecture of transformers, detailing their fundamental components and operational principles. Section 2.2 delineates the progression of Large Language Models, outlining key milestones and innovations. Section 2.3 examines the advancements in LLM capabilities, highlighting their impact on the field of NLP.

% their architectural innovations, key advancements in capabilities, and the challenges they face in handling numerical data and reasoning tasks, particularly in the context of finance.

\section{The Transformer Architecture: A Paradigm Shift}
The introduction of the Transformer architecture by Vaswani et al. (2017) marked a pivotal moment in NLP, fundamentally changing the approach to sequence transduction problems. Unlike previous recurrent or convolutional neural networks, the Transformer relies solely on attention mechanisms, enabling more efficient processing of sequential data and facilitating the training of much larger models \cite{vaswani2023attentionneed}.

Key components of the Transformer architecture include:
\begin{itemize}

 \item Self-Attention Mechanism This allows the model to weigh the importance of different parts of the input sequence when processing each element, enabling capture of long-range dependencies more effectively than previous architectures.

 \item Multi-Head Attention By applying multiple attention operations in parallel, the model can capture different types of relationships within the data simultaneously.
\item Positional Encoding: To compensate for the lack of inherent sequential processing, positional encodings are added to input embeddings, allowing the model to leverage sequence order.

 \item Feed-Forward Networks These process the outputs of the attention layers, adding non-linearity and increasing the model's capacity to learn complex functions.

 \item Layer Normalization and Residual Connections These components facilitate training of deep networks by stabilizing the learning process and mitigating the vanishing gradient problem.
\end{itemize}
The Transformer's ability to process input sequences in parallel, rather than sequentially, led to significant improvements in training efficiency and model performance. This architecture laid the groundwork for the development of increasingly large and sophisticated language models.

\section{Evolution of Large Language Models}

Building upon the Transformer architecture, researchers developed a series of increasingly powerful language models:

\subsection*{BERT: Bidirectional Encoders}

BERT (Bidirectional Encoder Representations from Transformers), introduced by Devlin et al. (2018), represented a significant advancement in pre-training techniques \cite{devlin2019bertpretrainingdeepbidirectional}. 

Key innovations of BERT include:

\begin{itemize}
 
   \item Bidirectional Context: Unlike previous models that processed text either left-to-right or right-to-left, BERT considers both left and right context simultaneously, enabling a more nuanced understanding of language.
   \item Masked Language Model (MLM) Pre-training: BERT is trained to predict masked words in a sentence, forcing it to learn contextual representations of words.
   \item Next Sentence Prediction: This additional pre-training task helps BERT understand relationships between sentences.
\end{itemize}

BERT's approach led to state-of-the-art performance across a wide range of NLP tasks, demonstrating the power of large-scale, unsupervised pre-training.

\subsection*{GPT Series: Scaling Up}

The GPT (Generative Pre-trained Transformer) series, developed by OpenAI, pushed the boundaries of model size and capabilities:
\begin{itemize}
 
   \item GPT : Introduced the concept of fine-tuning a large, pre-trained language model for specific downstream tasks \cite{radford2018improving}.
   \item GPT-2 : Scaled up the model size significantly (1.5 billion parameters) and demonstrated impressive text generation capabilities \cite{radford2019language}.
   \item GPT-3 : With 175 billion parameters, GPT-3 marked a leap in scale and showcased strong few-shot learning abilities, reducing the need for task-specific fine-tuning \cite{brown2020language}.
\end{itemize}
The GPT series highlighted the benefits of scale in language models, showing that larger models could exhibit more flexible and generalizable language understanding and generation capabilities.

\subsection*{Other Notable Models}
\begin{itemize}

    \item T5 : Unified various NLP tasks into a single text-to-text format, demonstrating the versatility of the transformer architecture \cite{raffel2020exploring}.
    \item BART : Combined the bidirectional encoder of BERT with the autoregressive decoder of GPT, showing strong performance on both text comprehension and generation tasks \cite{lewis2019bart}.
\end{itemize}
These models further expanded the capabilities of LLMs, showing their potential to handle a diverse range of NLP tasks within a single architectural framework.

\section{Advancements in LLM Capabilities}
As LLMs evolved, several key advancements emerged:
\subsection*{Scaling Laws}
Kaplan et al. (2020) established important relationships between model size, dataset size, and computational budget \cite{kaplan2020scaling}. Their findings showed that model performance scales predictably with these factors, guiding the development of more efficient and powerful models. This work provided a theoretical foundation for the "bigger is better" approach in language model development.
\subsection*{Few-shot and Zero-shot Learning}
Brown et al. (2020) demonstrated with GPT-3 that sufficiently large models could perform tasks with minimal or no task-specific training examples \cite{brown2020language}. This capability, known as few-shot and zero-shot learning, opened new possibilities for creating more flexible and adaptable AI systems.
\subsection*{Instruction Tuning}
Wei et al. (2022) showed that fine-tuning models on diverse sets of instructions could significantly improve their ability to follow natural language prompts \cite{wei2022finetunedlanguagemodelszeroshot}. This technique, known as instruction tuning, enhanced the versatility of LLMs across various tasks and made them more aligned with human intentions.
\subsection*{Chain-of-Thought Prompting}
Another significant advancement came from Wei et al. (2022), who demonstrated that prompting models to generate step-by-step reasoning could substantially improve their performance on complex tasks, including those involving numerical reasoning \cite{wei2022chain}. This technique, called chain-of-thought prompting, showed that LLMs could be guided to break down complex problems into more manageable steps, mirroring human problem-solving processes.
\subsection*{Reinforcement Learning from Human Feedback (RLHF)}
Ouyang et al. (2022) introduced techniques to align language models with human preferences using reinforcement learning \cite{ouyang2022training}. This approach, known as RLHF, led to the development of models that could produce more helpful, safe, and contextually appropriate outputs.
% A dissertation usually contains several chapters.

\chapter{Related Work}

The application of Large Language Models (LLMs) in the financial sector has been a subject of intense research in recent years, with studies exploring their potential and limitations across various financial tasks. This chapter reviews key works in this field, highlighting methodologies, findings, and persistent research gaps.
General-purpose LLMs like GPT-3, ChatGPT, and GPT-4 have demonstrated broad capabilities across various financial tasks. Li et al. (2023) conducted a comprehensive evaluation of these models using multiple benchmark datasets \cite{li2023large}. Their methodology involved fine-tuning these models on financial data and comparing their performance against specialized financial models. While the general-purpose LLMs often outperformed specialized models in tasks like sentiment analysis and named entity recognition, they struggled with complex financial reasoning tasks. This highlights a significant research gap: the need for LLMs that can perform deep, domain-specific financial analysis while maintaining their general language understanding capabilities.
% Aziz et al. (2023) further explored this gap by investigating the performance of GPT-3.5 and GPT-4 in financial sentiment analysis and market prediction tasks [2]. Their methodology involved prompting the models with historical market data and news headlines to generate price predictions. While the models showed promise in sentiment analysis, their performance in market prediction was inconsistent, especially in volatile market conditions. This underscores another research gap: the need for LLMs that can effectively process temporal financial data and adapt to rapidly changing market conditions.
Recognizing these limitations, researchers have begun developing models specifically tailored for the financial domain. Wu et al. (2023) introduced BloombergGPT, a 50-billion parameter language model trained on a vast corpus of financial data \cite{wu2023bloomberggptlargelanguagemodel}. Their approach involved pretraining the model on a mix of general and financial text data, followed by fine-tuning on specific financial tasks. BloombergGPT demonstrated significant improvements over general-purpose models in tasks such as financial sentiment analysis and question answering. However, the authors noted that the model still struggled with complex numerical reasoning, highlighting a persistent gap in LLMs' ability to perform accurate financial calculations consistently.
Wang et al. (2023) proposed FinGPT, leveraging instructional tuning to adapt LLMs for financial applications \cite{wang2023fingptinstructiontuningbenchmark}. Their methodology involved fine-tuning LLMs on a diverse set of financial instructions and tasks. The authors emphasized the importance of systematic evaluation across various financial competencies, from basic tasks to complex multitasking scenarios. While FinGPT showed improvements on text-based financial data, it overlooked other critical financial data forms such as numerical and tabular data, which are pivotal for comprehensive financial analysis.

Efforts to enhance LLM capabilities for finance have taken various forms. Chen et al. (2022) introduced the CONVFINQA dataset, focusing on complex numerical reasoning in conversational finance \cite{chen2022convfinqa}. Their work involved creating a dataset of multi-turn financial conversations that require chained reasoning over numbers. They experimented with both neural-symbolic approaches and prompting-based methods, finding that even state-of-the-art LLMs struggled with complex, multi-step financial calculations. This work highlights a critical research gap: the need for LLMs that can perform reliable, multi-step numerical reasoning in financial contexts.
Addressing the challenge of improving LLMs' instruction-following capabilities, Wang et al. (2023) proposed Self-Instruct, a method for enhancing LLMs through self-generated instructions \cite{wang2023selfinstruct}. While not specifically focused on finance, their approach of using LLMs to generate their own training data could potentially be applied to financial domains. The authors demonstrated a 33 percent improvement in GPT-3's general instruction-following capabilities. However, they noted limitations in the quality and diversity of self-generated instructions, pointing to a research gap in developing more sophisticated self-improvement mechanisms for LLMs.
Araci (2023) introduced FinBERT, a BERT-based model specifically designed for financial sentiment analysis \cite{araci2019finbertfinancialsentimentanalysis}. The author's methodology involved pretraining BERT on a large corpus of financial texts and fine-tuning it on labeled financial sentiment data. While FinBERT showed improvements over general-purpose models in sentiment classification tasks, it was limited to sentiment analysis and did not address more complex financial reasoning tasks, highlighting the need for more versatile financial LLMs.
The challenge of context modeling and reasoning in LLMs, crucial for many financial applications, was addressed by Zhao et al. (2023) in their work on natural language-based context modeling and reasoning with LLMs \cite{xiong2023naturallanguagebasedcontext}. They outlined various strategies for improving LLMs' ability to understand and utilize context, including prompt engineering, few-shot learning, and retrieval-augmented generation. While their work was not finance-specific, their findings have significant implications for financial applications. For instance, improved context modeling could enhance LLMs' ability to understand complex financial narratives or multi-turn financial conversations. 
Addressing the computational challenges of fine-tuning large models for specific tasks, Dettmers et al. (2023) proposed QLoRA (Quantized Low-Rank Adaptation), a parameter-efficient fine-tuning method \cite{dettmers2024qlora}. Their approach involves quantizing the pretrained language model to 4 bits, adding trainable low-rank adapters, and using a novel preconditioner which they term "Paged Optimizers" to handle GPU memory constraints. This method could potentially allow for more efficient adaptation of large, general-purpose LLMs to specific financial tasks without the need for extensive computational resources. 
% However, the authors noted that while QLoRA maintained performance comparable to full fine-tuning, there were still challenges in adapting models to tasks requiring specialized knowledge, pointing to a persistent gap in efficiently transferring domain-specific financial knowledge to LLMs.
Recent advancements in LLM capabilities have opened new avenues for their application in finance. Schick et al. (2023) introduced the Toolformer approach, which enables language models to learn to use external tools through self-supervised learning \cite{schick2023toolformer}. Their methodology involved augmenting a pretraining dataset with tool-use examples, allowing the model to learn when and how to call external tools. While not specifically focused on finance, this approach has significant implications for financial applications. For instance, a Toolformer-like model could potentially learn to access real-time market data, financial calculators, or regulatory databases, addressing the research gap of integrating external financial tools and data sources with LLMs. However, the authors noted challenges in tool selection and result interpretation, highlighting a persistent gap in LLMs' ability to reason about tool outputs more so in a complex domains like finance.
% The ethical and regulatory implications of using LLMs in finance were examined by Barr et al. (2022) [9]. Their review raised concerns about the potential for AI models to perpetuate biases or make decisions that could have significant economic impacts. This work underscores a critical research gap: the need for LLMs that can navigate complex financial regulations and adhere to ethical standards in financial decision-making.
% Tan et al. (2023) explored the use of LLMs for automated financial report generation [10]. Their methodology involved fine-tuning GPT-3 on a dataset of financial reports and testing its ability to generate coherent and accurate reports from financial data. While the model showed promise in generating human-like reports, the authors noted issues with numerical consistency and factual accuracy, highlighting the need for LLMs that can maintain numerical precision over long-form financial text generation.
% The issue of explainability in AI-driven financial analysis was explored by Linardatos et al. (2021) [7]. Their comprehensive review of interpretability methods in machine learning highlighted the unique challenges posed by the complexity of LLMs. The authors emphasized the need for explainable AI in finance, pointing to a significant research gap: developing LLMs that can not only provide accurate financial insights but also explain their reasoning in a way that is transparent and comprehensible to financial professionals.
% In conclusion, while LLMs have shown considerable promise in the financial domain, significant challenges remain. The reviewed works collectively highlight several critical research gaps:

% Developing LLMs that can perform deep, domain-specific financial analysis while maintaining general language understanding.
% Improving LLMs' ability to process temporal financial data and adapt to rapidly changing market conditions.
% Enhancing LLMs' capacity for complex, multi-step numerical reasoning in financial contexts.
% Creating explainable financial LLMs that can articulate their decision-making processes transparently.
% Designing LLMs that can adhere to financial regulations and ethical standards.
% Improving the numerical consistency and factual accuracy of LLMs in long-form financial text generation.

% Addressing these gaps will be crucial for the development of LLMs that can truly transform financial analysis and decision-making processes.

% Talk about PEFT, QLORA etc
\chapter{Methodology}
This chapter describes the methodology applied in this project. 
% Section 4.1 defines the dataset selection and preparation

\section{Dataset Selection and Preparation}

\subsection{Merged Dataset Creation}

We have used a mixture of four different financial datasets. The brief summary of these dataset is provided below. The structured sample for each of these dataset after preprocessing is provided in the appendix section. 

\textbf{TAT-QA } contains 16,552 questions associated with 2,757 hybrid contexts from real-world financial reports. The questions typically require a range of data extraction and numerical reasoning skills, including multiplication, comparison, sorting, and their various combinations \cite{zhu-etal-2021-tat}.

\textbf{ConvFinQA } contains 3,892 conversations consisting 14,115 questions from real-world scenario of conversational question answering over financial reports. The dataset is formulated using both textual content and structured table \cite{chen2022convfinqa}. 

\textbf{FINQA } is an expert annotated dataset that contains 8,281 financial QA pairs, along with their numerical reasoning processes. The reasoning processes answering these questions are made of many common calculations in financial analysis, such as addition, comparison, and table aggregation \cite{chen2021finqa}. 

\textbf{FinGPT FinRED-RE } dataset available on Hugging Face is designed for financial relationship extraction tasks. It contains 13.5k rows of data, divided into 11.4k training rows and 2.14k test rows. The dataset features text inputs with associated financial entities and their relationships. The task contains instructions for extracting financial relationships from textual data, utilizing specific relations like employer, industry, and product/material produced \footnote{https://huggingface.co/datasets/FinGPT/fingpt-finred-re}. 
% \begin{itemize}
%  \item Datasets Selected To ensure a comprehensive representation of the financial domain, four datasets were chosen:
%  \begin{itemize}
%      \item TAT-QA: A dataset focused on tabular financial data.
%      \item ConvFINQA: A financial question-answering dataset that includes both tables and texts, allowing for diverse data formats.
%      \item FinQA: Another financial question-answering dataset aimed at enhancing the breadth of question types and data structures.
%      \item Relation Extraction Dataset: A specialized dataset to assist in extracting relationships between different financial entities and concepts.
%  \end{itemize}
% \item 

\textbf{Merged dataset: } The datasets are merged to form a comprehensive unified dataset, balancing various types of financial data to ensure robust coverage of multiple scenarios and data formats encountered in the financial domain. This integration includes tabular data, text-based financial reports, conversational question-answer pairs, and extracted relationships, enhancing the dataset's versatility and applicability for a wide range of financial data analysis tasks. This comprehensive approach improves the ability to analyze diverse financial situations and generalize across tasks, making it valuable for both financial domain applications and general numerical reasoning scenarios.
% \end{itemize}
\subsection{Dataset Preprocessing}
\begin{itemize}
    \item Algorithm Development: A preprocessing algorithm~\ref{alg:preprocess_financial} was created to prepare the merged dataset for model training. This algorithm ensures that the data follows a unified structure comprising context, questions, and answers.
    \item Prompt Template: The data was formatted into a specific template prompt format required by both Llama2 7b and Llama3 8b models that are used in this project. This involved:
    \begin{itemize}
        \item Data Cleaning: Removing inconsistencies and irrelevant information.
        \item Normalization: Standardizing numerical values and textual content to maintain uniformity.
        \item Prompt Structuring: Organizing the data into the structured prompts that the models were originally trained on, ensuring compatibility and effectiveness during fine-tuning. This include use of special tokens and prompt structure of the specific model \footnote{https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3}. 
    \end{itemize}
\end{itemize}
\begin{algorithm}
\caption{Preprocessing Merged Dataset}
\label{alg:preprocess_financial}
\begin{algorithmic}[1]
\Procedure{PreprocessDatasets}{}
    \State Load TAT-QA, ConvFinQA, FinQA, and RelEx datasets
    \State $combined\_data \gets \emptyset$
    \For{$dataset \in \{TAT\text{-}QA, ConvFinQA, FinQA, RelEx\}$}
        \State $processed \gets $ \Call{ProcessDataset}{$dataset$}
        \State $combined\_data \gets combined\_data \cup processed$
    \EndFor
    \State $combined\_data \gets$ \Call{RandomSample}{$combined\_data, \lfloor |combined\_data| / 3 \rfloor$}
    \State Split $combined\_data$ into $train\_data$ (90\%) and $test\_data$ (10\%)
    \State \Return $train\_data, test\_data$
\EndProcedure

\Procedure{ProcessDataset}{$dataset$}
    \State $processed \gets \emptyset$
    \For{each $sample \in dataset$}
        \State $(context, question, answer) \gets$ \Call{ExtractInfo}{$dataset, sample$}
        \State $processed \gets processed \cup \{(context, question, answer)\}$
    \EndFor
    \State \Return $processed$
\EndProcedure

\Procedure{ExtractInfo}{$dataset, sample$}
    \If{$dataset = TAT\text{-}QA$}
        \State $context \gets$ Combine paragraphs and format table
        \For{each $q \in sample.questions$}
            \State \Return $(context, q.question, q.answer)$
        \EndFor
    \ElsIf{$dataset = ConvFinQA$}
        \State \Return $(sample.input, sample.instruction + \text{ instructions}, sample.output)$
    \ElsIf{$dataset = FinQA$}
        \State $context \gets$ Combine pre\_text, post\_text, and table
        \State \Return $(context, sample.qa.question, sample.qa.answer)$
    \ElsIf{$dataset = RelEx$}
        \State \Return $(sample.input, sample.instruction, sample.output)$
    \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Baseline Model Selection and Evaluation}
\begin{itemize}
    \item Baseline Models Used:
    \begin{itemize}
        \item meta-llama/Llama-2-7b-hf \footnote{https://huggingface.co/meta-llama/Llama-2-7b-hf}
        \item meta-llama/Meta-Llama-3-8B \footnote{https://huggingface.co/meta-llama/Meta-Llama-3-8B}
    \end{itemize}
    \item Performance Testing: The merged dataset was tested on these baseline models to establish initial performance metrics. This step was crucial for:
    \begin{itemize}
        \item Benchmarking: Establishing a reference point for comparing improvements post fine-tuning.
        \item Identifying Weaknesses: Understanding the initial strengths and weaknesses of the models with raw financial data.
    \end{itemize}
\end{itemize}
\section{Fine-tuning Approaches}
% \textbf{Objective:} To enhance model performance on the specific task of financial question answering and relation extraction.

\subsection{Training Procedure}
\subsubsection*{Model Architecture} We employed the Meta-Llama-2-7B-hf and Meta-Llama-3-8B model, a variant of the LLaMA architecture specifically adapted for few-shot learning scenarios. This choice was driven by its recent success in various NLP tasks, particularly those requiring nuanced understanding and generation based on limited context. For the code generation and execution task, we have used the recently launched Meta-Llama-3.1-8B model due to its enhanced ability for code generation tasks.

\subsubsection*{Dataset Preparation} For the first experiment of fine tuning, We curated a dataset from multiple sources as mentioned in section 4.1, aiming to encompass a wide range of financial topics to enhance the model's ability to generalize across diverse financial contexts. The data was split into 90\% for training and 10\% for validation, ensuring a representative distribution of topics in each subset.

For the X experiment, we exclusively used the TAT-QA dataset, following a preprocessing strategy similar to the one outlined in the algorithm~\ref{alg:preprocess_financial}, specifically steps 20-25.

\subsubsection*{Training Details}
The model was fine-tuned using a quantization-aware training approach, utilizing 4-bit quantization to balance performance with computational efficiency. The BitsAndBytes library facilitated the integration of low-precision arithmetic during training. Quantization is a two-step process, involves normalizing constants to scale vectors into a target range and rounding to the nearest target value. This technique can cause significant quantization loss if weights have outliers. To address this, \textit{bitsandbytes} employs vector-wise quantization and mixed precision decomposition, maintaining performance akin to non-quantized states \cite{j2024finetuningllmenterprise}. Although LLM int8 does not impair performance, it increases inference time due to quantization overhead but significantly reduces memory usage by 71\%, which enabled us to fine tune the models on NVIDIA GPUs.
Parameter-Efficient Fine-Tuning (PEFT) enhances the performance of pre-trained language models for specific tasks in Natural Language Processing. By adjusting only a subset of the model’s parameters on smaller datasets, PEFT conserves computational resources and time. This method typically involves freezing several layers of the model and fine-tuning only the final layers that directly pertain to the target application, thereby achieving greater efficiency \cite{j2024finetuningllmenterprise}. Low-Rank Adaptation (LoRA) \footnote{https://huggingface.co/docs/diffusers/en/training/lora} is an efficient training technique for large language models that significantly reduces the number of trainable parameters by inserting a smaller set of new weights, which are the only parts trained. This method speeds up training, enhances memory efficiency, and results in much smaller model weights (only a few hundred megabytes), making the models easier to manage and distribute. We employed the LoraConfig from the PEFT library, setting a rank of 16 and an alpha of 64 to finely tune the attention mechanism to our dataset while minimizing hardware needs without extra inference latency. In our experiment fine-tuning the Llama-3-8b model, the original parameter count was 4,582,543,360. With LoRA, we reduced it to 41,943,040 trainable parameters, constituting just 0.915\% of the total parameters. This reduction underscores the efficiency of LoRA in managing computational resources.

% all params: 4,582,543,360 || trainable params: 41,943,040 || trainable%: 0.9152786281546499

In our training setup, we employed a distributed system featuring eight NVIDIA GeForce RTX 3090 GPUs, leveraging PyTorch's DistributedDataParallel (DDP) framework. This choice was informed by insights from the work by Shen Li et. al (2020) which demonstrated DDP's superiority in synchronizing gradients efficiently across multiple GPUs. They noted that DDP minimizes communication overhead and optimizes computation by overlapping gradient reduction with backpropagation \cite{li2020pytorchdistributedexperiencesaccelerating}. This results in enhanced training speed and scalability, crucial for handling large datasets and complex models in a distributed environment. This approach was particularly necessary for fine-tuning Llama models in our resource-constrained environment, where utilizing multiple GPUs for data-parallel training was essential. 

Gradient accumulation \footnote{https://huggingface.co/docs/accelerate/en/usage\_guides/gradient\_accumulation} allows for the use of larger batch sizes than those limited by hardware memory by summing up gradients over multiple mini-batches and updating the model only after a predefined number of these batches. In our training setup, we managed a batch size of one per device, accumulating gradients over 12 steps. This strategy effectively simulates training with larger batch sizes, optimizing the trade-off between memory usage and training convergence speed.

AdamW is an optimization algorithm that modifies the classic Adam optimizer by decoupling weight decay from the gradient updates \cite{loshchilov2019decoupledweightdecayregularization}. This adjustment allows for more effective and theoretically sound management of weight decay, improving generalization compared to standard weight decay in optimizers like Adam. The modification ensures that the weight decay is applied directly to the weights themselves rather than as part of the gradient descent, which helps in better preserving the training stability and often leads to better performance on validation and test datasets. We utilized the AdamW optimizer with a learning rate of \textsc{2e-4}, chosen based on preliminary testing to ensure rapid convergence without compromising stability. The model underwent training over five epochs, incorporating early stopping based on validation loss to mitigate overfitting, thus enhancing model generalizability and performance efficiency.

In the code generation and execution experiment conducted on a Google Colab platform with a single NVIDIA A100 40GB GPU, the recently introduced \textsc{meta-llama/Meta-Llama-3.1-8B} model was deployed to generate Python code for financial analysis. This experiment was distinctive as it did not involve model fine-tuning. Instead, few-shot learning techniques were utilized, which included an enhanced prompt template featuring context, a specific question, and Python code examples, formatted with actual newline characters. To identify the most contextually relevant examples for the model, TF-IDF vectorization \cite{8999168} and cosine similarity metrics were applied. This methodological choice was influenced by the findings of Omid et al. (2019), who observed that despite the prevalent expectation favoring advanced topic models (e.g., Latent Semantic Indexing) and neural models (e.g., Paragraph Vectors) for superior performance across all measures of textual similarity, TF-IDF demonstrated remarkable efficacy. Specifically, TF-IDF excelled in scenarios involving longer and more technical texts and in making more nuanced distinctions among nearest neighbors. This attribute proved particularly advantageous for the current study, which involves complex financial datasets, thereby necessitating a method capable of handling the intricacies and technicalities embedded within such texts.

%significantly improving the model’s capacity to generate precise and executable code.

\subsubsection*{Evaluation Strategy}
Model performance was periodically evaluated on the validation set at the end of a predefined step in the training setup.

%This section of a paper would typically be accompanied by citations to relevant works, particularly for the methods used (like distributed training frameworks, few-shot learning techniques, etc.), and could include sub-sections with more detailed explanations of the dataset, model configuration, and specific training parameters if needed. Additionally, including figures or tables with interim performance metrics or final evaluation results could further enrich the section.

\subsubsection*{Few-Shot Example Configuration}
In Section 5.3.3, we detail the outcomes of the experiment involving few-shot prompting techniques, where we evaluated various few-shot learning methods. Few-shot learning (FSL) involves training models to recognize categories from very limited examples. One-shot learning (OSL) refers to learning tasks where only a single example per rare category is available. Zero-shot learning (ZSL), meanwhile, deals with categories for which no examples are provided \cite{billion2024low}. Suvarna et al. (2019) discusses the challenges and techniques of generalization in few-shot learning, emphasizing that while machines need numerous examples to learn like humans, they lack certain cognitive functions. In contrast to earlier methods requiring human intervention for learning representations, modern deep learning autonomously learns these representations. However, learning effective representations from few examples remains difficult. Deep models require diverse, \textit{representative examples} to generalize well but often struggle in few-shot scenarios due to the scarcity of such examples \cite{kadam2020review}.
To tackle the challenges of few-shot learning, we developed a strategy that utilizes both relevant and random examples. The selection of relevant examples is based on cosine similarity \footnote{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine\_similarity.html} measurements between the embeddings of training samples and a predefined set of few-shot examples. These embeddings are generated using the SentenceTransformer model, specifically \textit{all-MiniLM-L6-v2} \footnote{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2} which provides a dense representation of text ideal for similarity assessments. The selection process is detailed in Algorithm~\ref{alg:precomputeExamples}, where \textsc{top\_k} indicates the number of relevant samples calculated. In our experiments, \textsc{top\_k} is set to 2 due to memory constraints and the context length limitations of the Llama-2-7b-hf and Llama-3-8b models, which are 4096 and 8192 tokens, respectively. The parameters \textit{context\_weight} and \textit{question\_weight} are both set to 0.5, reflecting the equal importance of context and questions in datasets such as ConvFinQA, where the history and the final question are crucial. This approach ensures that no input is truncated during training, although it limits our ability to test performance with a larger number of examples, i.e., a much higher \textsc{top\_k}.

In the experiment for code generation detailed in section 5.4.1, we used few-shot learning techniques, leveraging an improved prompt template that included context, a question, and Python code examples, with a focus on proper formatting using actual newline characters.
To select the most relevant examples for the model, we employed TF-IDF vectorization and cosine similarity measures. This approach ensured that the examples used in the prompt were highly pertinent to the task at hand, enhancing the model's ability to generate accurate and executable code.


\begin{algorithm}
\caption{Precompute Relevant Examples for Few-Shot Learning}
\label{alg:precomputeExamples}
\begin{algorithmic}[1]
\Function{GetEmbeddings}{text}
    \State inputs $\gets$ embedding\_tokenizer(text, return\_tensors='pt', padding=True, truncation=True)
    \State embedding $\gets$ embedding\_model(**inputs).last\_hidden\_state.mean(dim=1).cpu().numpy()
    \State \Return embedding[0]
\EndFunction

\Function{PrecomputeRelevantExamples}{dataset, few\_shot\_examples, top\_k, context\_weight, question\_weight}
    \State few\_shot\_context\_embeddings $\gets$ [GetEmbeddings(ex["context"]) for ex in few\_shot\_examples]
    \State few\_shot\_question\_embeddings $\gets$ [GetEmbeddings(ex["question"]) for ex in few\_shot\_examples]
    \State relevant\_examples\_map $\gets$ \{\}
    \For{idx, item in enumerate(dataset)}
        \State context\_embedding $\gets$ GetEmbeddings(item['context'])
        \State question\_embedding $\gets$ GetEmbeddings(item['question'])
        \State context\_similarities $\gets$ cosine\_similarity([context\_embedding], few\_shot\_context\_embeddings)[0]
        \State question\_similarities $\gets$ cosine\_similarity([question\_embedding], few\_shot\_question\_embeddings)[0]
        \State combined\_similarities $\gets$ context\_weight * context\_similarities + question\_weight * question\_similarities
        \State most\_relevant\_indices $\gets$ argsort(combined\_similarities)[-top\_k:][::-1]
        \State relevant\_examples\_map[str(idx)] $\gets$ most\_relevant\_indices.tolist()
    \EndFor
    \State \Return relevant\_examples\_map
\EndFunction

% \Function{SaveRelevantExamples}{relevant\_examples\_map, filename}
%     \State Open filename as f
%     \State json.dump(relevant\_examples\_map, f)
% \EndFunction

% \Function{LoadRelevantExamples}{filename}
%     \State Open filename as f
%     \State \Return json.load(f)
% \EndFunction

\end{algorithmic}
\end{algorithm}

\subsection{Post Processing Algorithm}

During inference, the model generates responses based on varying prompt templates, which differ across experiments and models. Even with the same model, the prompts vary depending on whether we use few-shot prompts or additional chain-of-thought prompting to guide the model to generate explanations. Consequently, a robust post-processing algorithm is necessary. Algorithm~\ref{alg:postprocess_output} presents a generic post-processing approach used during inference and evaluation, with slight modifications based on the model's output response. The code for all post-processing steps is available in my Github Repo\footnote{https://github.com/rjanant/disseration}.
\begin{algorithm}
\caption{Post-Processing Model Output}
\label{alg:postprocess_output}
\begin{algorithmic}[1]
\Function{PreprocessOutput}{output}
    \State \textbf{Input:} Model's output string
    \State \textbf{Pattern Matching:} Use regex to find the answer section
    \State answer\_pattern $\gets$ 
    \text{r"Answer (including calculation steps and final answer,}
    \State \text{use '\\n' for line breaks):(.*?)}
    \State \text{(?:\\n\\nContext:|\$)"}
    \State match $\gets$ \texttt{re.search(answer\_pattern, output, re.DOTALL)}
    \If{match is not None}
        \State answer $\gets$ match.group(1).strip()
        \State lines $\gets$ [line.strip() for line in answer.split('\textbackslash n') if line.strip()]
        \State \Return lines[0] \textbf{if} lines \textbf{else} ""
    \Else
        \State \Return ""
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}
% \textbf{Purpose: }To refine the generated outputs, especially in relation extraction where over-extraction was an issue.
% \vspace{1}
% \textbf{Algorithm Features: }
% \begin{itemize}
%     \item Filtering Mechanism: Implementing rules to discard irrelevant extractions.
%     \item Relevance Scoring: Scoring extracted values to prioritize and retain only the most relevant ones.
% \end{itemize}

\section{Evaluation and Iteration}
\subsection{Permissive Accuracy}
We have designed a permissive accuracy measure that accommodates minor precision differences by allowing slight deviations in decimal points within an alpha threshold. This measure is particularly useful in financial contexts, where exact numerical precision may not always be critical. The method involves extracting numerical values and comparing them within a tunable alpha difference.

For threshold setting, we define acceptable deviation ranges, ensuring flexibility in various applications. This measure is especially relevant in scenarios like relational extraction, where generated outputs are textual values. In such cases, the SequenceMatcher is used for string comparison, returning 1 if the similarity exceeds $\beta$ \footnote{$\beta$ = 0.9 \& \alpha = 0.001}  or if key parts of the prediction and reference match. This approach is beneficial when the model generates the correct answer but includes additional values, which can be ignored. Additionally, we have an exact match function to report precise match accuracy, ensuring comprehensive evaluation of model performance.
\begin{algorithm}
\caption{Permissive Accuracy}
\label{alg:calculate_accuracy_permissive}
\begin{algorithmic}[1]
\Function{Permissive\_Accuracy}{predicted, actual}
    \State \textbf{Input:} predicted, actual
    \State predicted $\gets$ \texttt{str(predicted).lower().strip()}
    \State actual $\gets$ \texttt{str(actual).lower().strip()}
    
    \State pred\_num $\gets$ \texttt{ExtractNumericalValue(predicted)}
    \State actual\_num $\gets$ \texttt{ExtractNumericalValue(actual)}
    
    \If {pred\_num is not None and actual\_num is not None}
        \State \textbf{Return} \texttt{int(abs(pred\_num - actual\_num) < \alpha)}
    \Else
        \State pred\_words $\gets$ \texttt{predicted.split()}
        \State actual\_words $\gets$ \texttt{actual.split()}
        \State similarity $\gets$ \texttt{SequenceMatcher(None, pred\_words, actual\_words).ratio()}
        \State key\_parts\_match $\gets$ \texttt{all(part in predicted for part in actual.split(':')[-1].split(','))}
        \State \textbf{Return} \texttt{int(similarity > 0.9 or key\_parts\_match)}
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}
\subsection{Exact Match Accuracy}
In the context of financial datasets, exact match accuracy is vital for ensuring the precision of numerical output generation. Given the importance of accuracy in financial analysis, even a minor deviation, such as a difference in decimal points, is unacceptable and considered an error. This metric enforces a strict criterion by directly comparing the generated output with the expected answer, ensuring only perfectly matching results are considered correct. 
% \begin{itemize}
%     \item Importance: Maintaining a strict criterion for scenarios where exact answers are necessary like in case of numerical output generations where even a decimal point difference will be considered as an error. 
%     \item Measurement: Comparing the generated output directly with the expected answer for an exact match.
% \end{itemize}
\subsection{Inference Phase Metrics}
\subsubsection{Comprehensive Evaluation: Using a suite of metrics to assess various aspects of model performance}
\begin{itemize}
    \item ROUGE: Measures overlap of n-grams between generated and reference texts.
    \item METEOR: Considers precision, recall, and synonymy for evaluating translation quality.
    \item BLEU Score: Evaluates the precision of n-grams in generated text against reference.
    \item BERT Precision, Recall, F1 Score: Uses contextual embeddings to measure semantic similarity.
    \item GLUE Benchmark: It is a collection of resources for training, evaluating, and analyzing natural language understanding systems across diverse tasks.
\end{itemize}
\section{External Tool Integration \& Few-Shot Code Generation}
%talk about the idea of using python code generated from the financial dataset input and context and using langchain's agent tool use to generate the financial answer. begin with implementing idea on a small handcrafted sample dataset to check if the idea will be feasible on the large financial datasets for improving numerical precision. Then the actual experiment - dataset i.e. TAT-QA, generated python script and using exec function of python to generate final answer. 

[\textcolor{blue}{\textit{\textbf{Ask potential research question and try to answer those questions in this section as well as in the experiments - Results and Discussion section | Also add diagram of this code generation approach in a well defined manner like the one on langgraph for code gen page | In Experiments talk about use of atmax 5 rel examples but couldn't due to model limit context length and on truncation losing useful context for specific dataset i.e. convfinqa - add sample in appendix from each dataset}}}]

Recent advancements in tackling complex reasoning tasks, have opened new avenues for applying large language models to specialized domains like financial analysis. Notably, the study by Suzgun et al.(2022) provides compelling evidence for the efficacy of step-by-step approaches in solving intricate problems. The study focused on BIG-Bench, a collaborative benchmark comprising over 200 diverse text-based tasks, including traditional NLP, mathematics, commonsense reasoning, and question-answering \cite{suzgun2022challenging}. Particularly relevant to our research is their curation of BIG-Bench Hard (BBH), a subset of 23 especially challenging tasks where previous models had not surpassed average human-rater performance.

The study's results are particularly illuminating: using various \textit{chain-of-thought} (CoT) approaches, they found that the Codex model with CoT prompting outperformed the average human rater score on 17 out of 23 chosen tasks. This remarkable performance on diverse, complex tasks underscores the potential of step-by-step reasoning approaches in tackling challenging problems. Drawing parallels to our research, we recognized that the complex nature of financial data analysis - involving intricate calculations, temporal considerations, and the integration of tabular and textual data - could benefit from a similar methodical approach.

Inspired by these findings, our research leverages a step-by-step code generation methodology to address the unique challenges posed by financial datasets. By breaking down complex financial analyses into discrete, programmable steps, we aim to enhance the accuracy and reliability of our model's outputs. This approach allows us to handle the multifaceted nature of financial data, including numerical precision in calculations, temporal dependencies in time-series data, and the integration of qualitative information from textual sources.

Our methodology extends the concept of chain-of-thought reasoning to the domain of financial analysis, where each step in the chain is represented by a generated code snippet. This not only facilitates more accurate computations but also provides transparency and interpretability in the analysis process - crucial factors in the financial sector where decisions often require clear justification and auditable processes.

Our experimental design commenced with the careful selection of a small, handcrafted sample derived from a diverse array of financial question-answer pairs. These samples were meticulously chosen from the comprehensive dataset detailed in Section 4.1.1, ensuring a representative cross-section of financial queries and their corresponding responses.

To enhance the analytical capabilities of our model, we implemented a novel approach: replacing the original answers in the dataset with Python scripts. These scripts were generated with the assistance of GPT-4, a state-of-the-art language model, and subsequently underwent rigorous manual verification to ensure accuracy and functionality. This transformation allowed for a more dynamic and computationally rich representation of financial data analysis. The sample dataset was intentionally constructed to encompass a wide spectrum of answer types, including Numerical responses, Textual answers and Temporal data. This diversity in response types was crucial for assessing the model's versatility in handling various financial data representations. For the execution of these Python scripts embedded within the answer fields, we leveraged the LangChain LLM framework integrated into our pipeline. The execution environment was facilitated by a Python agent interfacing with a Python REPL (Read-Eval-Print Loop) tool. The REPL tool was instrumental in enabling real-time execution and evaluation of Python code, a feature paramount for the dynamic and precise computation of financial metrics and analyses. The language model underpinning this integration was the \textsc{facebook/opt-1.3b model}, selected for its robust natural language processing capabilities and its ability to understand and generate context-aware responses. 
%Decoding strategies for code generation include deterministic methods like greedy search and beam search, and sampling techniques such as temperature sampling, top-k sampling, and top-p (nucleus) sampling \cite{jiang2024survey}. 
As demonstrated by \cite{holtzman2019curious}, Nucleus Sampling is highly effective for neural generation. By sampling from the dynamic nucleus of the probability distribution, it balances diversity and reliability, resulting in text that mirrors human quality while maintaining fluency and coherence. In our pipeline, we use parameters: temperature=0.7, top\_p=0.95, and repetition\_penalty=1.15. Using these parameters means controlling the randomness of the generation (temperature=0.7), focusing on the top 95\% of the probability mass (top\_p=0.95), and discouraging repetitive sequences (repetition\_penalty=1.15). The results are presented and analyzed in Chapter 5.

In light of the promising results obtained from the initial experiment, we proceeded to evaluate the latest \textsc{meta-llama/Meta-Llama-3.1-8B-Instruct} model, a cutting-edge language model. This model demonstrated superior performance in code generation on the HumanEval benchmark, surpassing the GPT-3.5 Turbo model, and also achieved comparable results on reasoning tasks using the ARC Challenge benchmark \footnote{https://ai.meta.com/blog/meta-llama-3-1/}, thereby establishing its suitability for our study.

The process begins with a dynamic few-shot prompting mechanism designed to optimize the model's responses to specific financial queries. To ensure contextual alignment, we utilize TF-IDF vectorization and cosine similarity to identify and select the most relevant examples from our dataset for each query. These selected examples are then integrated into an enhanced few-shot prompt template contained in~\ref{sec:Prompt template-code-gen}, along with the corresponding context and question, to guide the model in generating accurate Python code.

The code generation process is fine-tuned with parameters such as temperature (0.9) and top\_p (0.95) to strike a balance between creativity and coherence. The generated code is subsequently extracted using regex pattern matching and executed within a controlled environment. This environment is equipped with the necessary libraries and contextual data, facilitating comprehensive financial data manipulation and analysis.

To evaluate the effectiveness of our approach, we processed a randomly selected subset of 1,000 samples from the TAT-QA dataset, ensuring the representativeness of the sample. For each sample, Python code was generated, executed, and the output was compared against a reference answer. The evaluation utilized the same suite of metrics detailed in Section 4.4.3, ensuring a rigorous assessment of the code generation approach within the context of financial tasks. The flowchart is explained in Figure~\ref{fig:methodology-flow}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{mscdiss-skeleton/updated-financial-analysis-flowchart.jpg}
    \caption{LLaMA-3.1-8B Workflow: From Query to Code Execution}
    \label{fig:enter-label}
\end{figure}
%This methodology represents a novel approach to financial data analysis, combining the power of advanced language models with domain-specific knowledge and real-time code execution. By dynamically generating and executing code based on natural language queries, our system demonstrates the potential for more flexible and accurate financial analysis tools.

\begin{figure}[!ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[
    node distance = 0.8cm and 1.2cm,
    auto,
    thick,
    block/.style = {rectangle, draw, fill=white, text width=2cm, text centered, 
                    minimum height=0.8cm, font=\footnotesize, rounded corners},
    cloud/.style = {ellipse, draw, fill=white, text width=2cm, text centered, 
                    minimum height=0.8cm, font=\footnotesize},
    database/.style = {cylinder, draw, shape border rotate=90, aspect=0.25, fill=white, 
                       text width=2cm, text centered, minimum height=1.2cm, font=\footnotesize},
    line/.style = {draw, -{Stealth[scale=0.8]}, thick},
    dashed-line/.style = {draw, -{Stealth[scale=0.8]}, dashed, thick},
    group label/.style = {font=\tiny\bfseries, fill=white, draw, rounded corners, text centered, 
                          fill=gray!20, inner sep=1pt},
    group/.style = {rectangle, draw, rounded corners, fill=gray!10, inner sep=0.2cm}
]
% Input Processing
\node [cloud] (input) {Input Query};
\node [block, below=of input] (selection) {Example Selection};
\node [database, left=of selection, xshift=-0.8cm] (dataset) {TAT-QA Dataset};
% Code Generation
\node [block, below=of selection] (prompt) {Prompt Construction};
\node [block, below=of prompt] (generation) {Code Generation};
\node [cloud, left=of generation, xshift=-0.8cm] (model) {meta-llama/\\Meta-Llama-3.1-\\8B-Instruct};
\node [block, below=of generation] (extraction) {Code Extraction};
% Code Execution
\node [block, below=of extraction] (execution) {Code Execution};
\node [database, left=of execution, xshift=-0.8cm] (environment) {Execution Environment};
\node [block, below=of execution] (output) {Output Generation};
% Evaluation
\node [block, below=of output] (evaluation) {Evaluation of Results};
\node [database, left=of evaluation, xshift=-0.8cm] (metrics) {Metrics Suite};
% Connections
\path [line] (input) -- (selection);
\path [dashed-line] (dataset) -- (selection);
\path [line] (selection) -- (prompt);
\path [line] (prompt) -- (generation);
\path [dashed-line] (model) -- (generation);
\path [line] (generation) -- (extraction);
\path [line] (extraction) -- (execution);
\path [dashed-line] (environment) -- (execution);
\path [line] (execution) -- (output);
\path [line] (output) -- (evaluation);
\path [dashed-line] (metrics) -- (evaluation);
% Group boxes
\begin{pgfonlayer}{background}
    \node [group, fit=(input) (selection) (dataset)] (input-group) {};
    \node [group, fit=(prompt) (generation) (extraction) (model)] (generation-group) {};
    \node [group, fit=(execution) (output) (environment)] (execution-group) {};
    \node [group, fit=(evaluation) (metrics)] (evaluation-group) {};
\end{pgfonlayer}
% Group labels
\node [group label, above=0.05cm of input-group.north west, anchor=west] {Input Processing};
\node [group label, above=0.05cm of generation-group.north west, anchor=west] {Code Generation};
\node [group label, above=0.05cm of execution-group.north west, anchor=west] {Code Execution};
\node [group label, above=0.05cm of evaluation-group.north west, anchor=west] {Evaluation};
\end{tikzpicture}
}
\caption{Flowchart of the Code Generation Process for Financial Analysis}
\label{fig:methodology-flow}
\end{figure}

%In the initial setup we choose a small handcrafted sampled from a variety of financial question-answer samples chosen from the dataset explained in section 4.1.1 . We then replaced the actual answers in the dataset with python scripts generated using the help of GPT-4 and verified manually. The sample of dataset were diverse and contains different representations like numerical answers or textual answer or date etc. To execute the Python scripts included in the answer fields, we utilised the LangChain LLM from the pipeline and then used the Python agent to run through a Python REPL (Read-Eval-Print Loop) tool. The Python REPL tool allows for real-time execution of Python code, which was crucial for dynamic and accurate computation of financial data. The model utilized for this integration was the Facebook 1.5b model, known for its robust language processing capabilities.

%\subsection{Subset Selection and Preparation}
%First, a small subset of the dataset was curated specifically for the LangChain integration. This subset was chosen to represent a variety of financial question-answer scenarios that required detailed computational analysis. Each entry in this subset included Python scripts in the answer fields, which were necessary for executing complex calculations. The selection criteria ensured that the subset contained diverse and representative examples that could test the model’s ability to handle different types of financial queries.

%\subsection{Python REPL Pipeline Integration}
%To execute the Python scripts included in the answer fields, the subset was run through a Python REPL (Read-Eval-Print Loop) pipeline. The Python REPL pipeline allows for real-time execution of Python code, which is crucial for dynamic and accurate computation of financial data. The model utilized for this integration was the Facebook 1.5b model, known for its robust language processing capabilities.

%\subsection{Integration Process}

%The integration process involved several technical steps:

%\begin{itemize}
    %\item Embedding Python Scripts: The Python scripts were embedded directly into the model prompts. This required careful formatting to ensure the scripts were correctly interpreted and executed by the REPL pipeline.
    %\item Model Adaptation: The Facebook 1.5b model was adapted to recognize and handle the embedded Python scripts. This adaptation involved training the model to parse the script correctly and execute it within the context of the financial question-answering task.
    %\item Output Handling: The outputs generated by the Python scripts were then reintegrated into the model’s response. This involved capturing the output from the REPL pipeline, formatting it appropriately, and ensuring it was included in the final answer provided by the model.
%\end{itemize}

%\subsection{Execution and Validation}
%During the inference phase, the integrated system worked as follows:

%\begin{itemize}
   % \item Query Processing: When a financial query was posed, the model parsed the question and identified the need for computational analysis.
    %\item Script Execution: The relevant Python script was executed using the Python REPL pipeline. This real-time execution allowed the model to handle complex calculations dynamically.
    %\item Result Integration: The results from the script execution were captured and formatted to be part of the model’s response. This integration ensured that the answer provided was accurate and directly addressed the computational aspects of the query.
%\end{itemize}
% \section{Other approaches used}

\chapter{Experiments}

This chapter outlines the setup, methods, and findings of our study, which aims to improve how language models handle numbers in financial settings. We've explored several strategies, including instruction tuning, few-shot prompting, and fine-tuning, to outperform current models on a variety of financial tasks. We've also introduced an innovative approach: generating code from input and executing it through a Python exec function in a structured pipeline to boost the model’s ability to process numerical data. Detailed discussions about these techniques are provided in Section 5.4.

\section{Research Questions}
Our experiments were designed to address the following research questions:
\begin{itemize}
\item How do different model architectures (Llama2-7B vs. Llama3-8B) compare in their baseline performance on financial tasks?
\item What is the impact of fine-tuning on the models' performance across various metrics?
\item How effective are different few-shot prompting strategies (e.g., selective, random, chain-of-thought) compared to fine-tuning?
\item What is the optimal number and type of examples for few-shot prompting in financial tasks?
\item How does the performance of language models compare to specialized tool-based approaches in financial data processing?
\end{itemize}

\section{Experimental Setup}

\subsection{Dataset}
We utilized a composite dataset combining four financial datasets which are explained in section 4.1 in detail. This diverse dataset allows us to evaluate model performance across various financial data types and tasks.

\subsection{Models}

We experimented with the following models:
\begin{itemize}
\item Llama2-7B chat (baseline and fine-tuned)
\item Llama3-8B (baseline and fine-tuned)
\item Llama3.1-8b (baseline) 
\item Llama3.1-8b-instruct (baseline)
\end{itemize}

\subsubsection{Approaches}

We implemented and compared several approaches:

\begin{enumerate}
  \item Baseline performance (zero-shot)
  \item Fine-tuning
  \item Few-shot prompting with chain-of-thought (FSP + COT)
  \item Selective few-shot prompting
  \item Random few-shot prompting
  \item Tool-based approach using Langchain (on a subset of 150 samples)
   \item Code generation and execution (on TAT-QA dataset)
\end{enumerate}

\subsection{Evaluation Metrics}
We employed a comprehensive set of metrics to evaluate model performance which are all detailed in section 4.4.3:
\begin{itemize}
\item Average Permissive Accuracy (custom metric allowing for slight numerical differences and partial text matches)
\item Average Exact Accuracy
\item ROUGE Score
\item BLEU Score
\item METEOR Score
\item BERTScore (Precision, Recall, F1)
\item GLEU Score
\end{itemize}
\begin{figure}[h!] 
    \centering 
    \includegraphics[width=0.75\textwidth]{mscdiss-skeleton/Screenshot 2024-08-04 at 19.20.50.png} 
    \caption{Performance Metrics of various models} 
    \label{fig:my_label} 
\end{figure}

\section{Results and Analysis}
\subsection{Model Architecture Comparison}
Table 5.1 presents the evaluation metrics for various models and approaches.

\begin{table}[h!]
\centering
\caption{Evaluation Metrics for Various Models}
\label{table:metrics}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccccccc@{}}
\toprule
Model & Avg Permissive Acc. & Avg Exact Acc. & ROUGE Score & BLEU Score & METEOR Score & BERTScore P & BERTScore R & BERTScore F1 & GLEU Score \\ 
\midrule
%Llama2-7B chat baseline & 0.2720 & 0.0225 & 0.16 & 0.01 & 0.19 & 0.814 & 0.860 & 0.836 & 0.02 \\
Llama2-7B chat baseline & 0.27 & 0.02 & 0.16 & 0.01 & 0.19 & 0.814 & 0.860 & 0.836 & 0.02 \\
Llama3-8B baseline & 0.33 & 0.05 & 0.20 & 0.03 & 0.19 & 0.836 & 0.876 & 0.855 & 0.04 \\
Llama2-7B chat Fine tuned & 0.46 & 0.00 & 0.15 & 0.05 & 0.21 & 0.778 & 0.892 & 0.830 & 0.07 \\
Llama3-8B Fine tuned & 0.67 & 0.15 & 0.36 & 0.06 & 0.31 & 0.850 & 0.919 & 0.881 & 0.07 \\
Llama3-8B FSP + COT & 0.59 & 0.15 & 0.36 & 0.07 & 0.31 & 0.854 & 0.917 & 0.883 & 0.07 \\
Llama3-8B Selective FSP & 0.64 & 0.11 & 0.27 & 0.06 & 0.28 & 0.809 & 0.913 & 0.855 & 0.05 \\
Llama3-8B Random FSP & 0.64 & 0.11 & 0.27 & 0.06 & 0.28 & 0.807 & 0.912 & 0.854 & 0.05 \\
%LLAMA3-8B {Zero shot, one shot, five shot.. } & Separate table & - & - & - & - & - & - & - & - \\
%Tool use Langchain [150 samples] & - & 0.75 & 0.30 & 0.23 & 0.33 & 0.822 & 0.878 & 0.849 & 0.23 \\
\bottomrule
\end{tabular}
}
\end{table}
\textbf{Key observations:}
\begin{itemize}
 \item Llama3-8B baseline outperforms Llama2-7B chat baseline across all metrics, indicating that the newer architecture has inherently better number handling capabilities.
 \item The performance gap is particularly noticeable in Average Permissive Accuracy (0.33 vs. 0.27) and Average Exact Accuracy (0.05 vs. 0.02).
\end{itemize}
\subsection{Impact of Fine-tuning}
\textbf{Fine-tuning significantly improved the performance of both model architectures:}
\begin{itemize}

\item Llama2-7B chat: Average Permissive Accuracy increased from 0.27 to 0.46
\item Llama3-8B: Average Permissive Accuracy increased from 0.33 to 0.67

The fine-tuned Llama3-8B model showed the best overall performance, with notable improvements in ROUGE Score (0.36) and METEOR Score (0.31).
\end{itemize}

\subsection{Few-shot Prompting Strategies}
Different few-shot prompting strategies yielded varying results:
\begin{itemize}

\item FSP + COT: Achieved high performance (0.59 Average Permissive Accuracy) but did not surpass fine-tuned models.
\item Selective FSP and Random FSP: Both achieved 0.64 Average Permissive Accuracy, indicating that careful example selection may not always be necessary.
\end{itemize}

\subsection{Optimal Few-shot Prompting}
Table 5.2 shows the performance metrics for different few-shot prompting approaches.
%Change this table based on new experiments on random, selective and zero examples
\begin{table}[h!]
\centering
\caption{Performance Metrics for Different Few Shot Prompting}
\label{tab:performance_metrics}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccccccc@{}}
\toprule
Fine Tuning Type & Avg Permissive Acc. & Avg Exact Acc. & ROUGE Score & BLEU Score & METEOR Score & BERTScore P & BERTScore R & BERTScore F1 & GLEU Score \\ 
\midrule
1 Relevant Example & 0.62 & 0.12 & 0.32 & 0.06 & 0.28 & 0.841 & 0.914 & 0.874 & 0.06 \\
1 Random Example & 0.63 & 0.13 & 0.33 & 0.06 & 0.29 & 0.840 & 0.915 & 0.874 & 0.06 \\
Results 1 & 0.61 & 0.14 & 0.34 & 0.06 & 0.28 & 0.852 & 0.916 & 0.881 & 0.07 \\
Results 2 & 0.61 & 0.14 & 0.34 & 0.06 & 0.28 & 0.853 & 0.915 & 0.881 & 0.07 \\
Results 3 & 0.61 & 0.14 & 0.35 & 0.06 & 0.28 & 0.855 & 0.916 & 0.883 & 0.07 \\
\bottomrule
\end{tabular}
}
\end{table}

\textbf{Key findings:}
\begin{itemize}
 \item Using a single relevant example or a single random example yielded similar performance (0.62 vs. 0.63 Average Permissive Accuracy).
 \item The results suggest that the number of examples may be more important than their specific relevance in this context.
\end{itemize}

\subsection{Example Type Distributions}
Table 5.3 presents the distribution and weights of different example types used in the experiments.

The varied distributions across different result sets allow us to analyze the impact of example selection strategies on model performance.
\begin{table}[h!]
\centering
\caption{Example Type Distributions and Weights}
\label{tab:example_weights}
\begin{tabular}{>{\raggedright}p{2.5cm}cccc}
\toprule
\textbf{Results} & \textbf{Combined} & \textbf{Relevant } & \textbf{Randomized} & \textbf{Baseline} \\
\midrule
Results 1 & 341 (0.25) & 304 (0.25) & 325 (0.25) & 317 (0.25) \\
Results 2 & 147 (0.10) & 324 (0.25) & 499 (0.40) & 317 (0.25) \\
Results 3 & 338 (0.25) & 616 (0.50) & 197 (0.15) & 136 (0.10) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Tool-based Approach}

The tool-based approach using Langchain on a subset of 150 samples showed promising results:
\begin{itemize}
   \item Exact Accuracy: 0.75
   \item BLEU Score: 0.23
   \item METEOR Score: 0.33
\end{itemize}
The findings from this study underscore the efficacy of specialized tools in performing precise financial tasks, with a notable requirement for exact matches in data handling. It is pertinent to acknowledge that these results are derived from scenarios utilizing manually generated code, specifically using GPT-4 \footnote{https://chatgpt.com}, which represents a potentially idealized set of conditions.

An interesting continuation of this research could involve leveraging OpenAI's advanced code generation capabilities in conjunction with Langchain's Python agent REPL tool to automate and refine this process further. However, such an approach would entail additional costs, which were not within the purview of this current study. Nevertheless, the potential of commercial language models to adeptly manage the complexity and variability inherent in our financial dataset remains a compelling aspect of our ongoing investigation. This line of inquiry suggests substantial promise for enhancing automated financial analysis tools through the integration of sophisticated AI-driven technologies.

\section{Code Generation Approach}
In this series of experiments, we explore a novel approach to enhancing numerical precision in language models for financial tasks. Rather than relying on traditional fine-tuning methods, we leverage the models' inherent code generation capabilities through strategic few-shot prompting. This approach is particularly relevant in the context of financial data processing, where precise numerical handling is crucial.
Our motivation stems from the observation that while large language models have demonstrated impressive natural language understanding and generation capabilities, their performance in tasks requiring precise numerical computations often falls short. By guiding these models to generate executable code, we aim to bridge this gap and achieve higher accuracy in financial calculations.

\subsection{Experimental Setup}

\textbf{Dataset:} We utilized the TAT-QA dataset \cite{zhu-etal-2021-tat}, a benchmark for text-and-table question answering. To ensure a manageable yet representative sample, we randomly selected \textit{1000} samples from the dataset. This selection was kept constant across all models using a fixed seed value for reproducibility.

\textbf{Models:} We experimented with several models from the Llama family:
\begin{enumerate}
    \item Llama3-8b
    \item Llama3.1-8b
    \item Llama3.1-8b-instruct
\end{enumerate}

Additionally, we included results from a \textsc{facebook/opt-1.3b} model using Langchain for comparison, though this was tested on a smaller sample of 150 instances.

Our approach can be summarized in the following steps:
\begin{enumerate}
   \item Few-shot prompting: We provided the models with example samples that demonstrate the desired code structure.
   \item Code generation: The models were then prompted to generate similar code for new inputs.
   \item Code execution: We used Python's exec() function to execute the generated code and produce final results.
   \item Evaluation: The results were evaluated using various metrics, including permissive accuracy, exact accuracy, ROUGE, BLEU, METEOR, BERTScore, and GLEU.
\end{enumerate}
\begin{figure}[h!] 
    \centering 
    \includegraphics[width=0.75\textwidth]{mscdiss-skeleton/Screenshot 2024-08-04 at 21.03.15.png} 
    \caption{Performance Metrics of various models in Code Generation} 
    \label{fig:my_label} 
\end{figure}
It's worth noting that the Llama models used in this study, particularly the 8b variants, are not primarily known for their code generation capabilities. This choice was deliberate, as it allows us to test the limits of our approach and potentially demonstrate its efficacy even with models not specialized for this task.

\subsection{Results and Analysis}
Table 5.4 presents the evaluation metrics for various models and approaches.
\begin{table}[h!]
\centering
\caption{Evaluation Metrics for Code Generation}
\label{table:metrics}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccccccc@{}}
\toprule
Model & Avg Permissive Acc. & Avg Exact Acc. & ROUGE Score & BLEU Score & METEOR Score & BERTScore P & BERTScore R & BERTScore F1 & GLEU Score \\ 
\midrule
facebook/opt-1.3b (langchain - Test samples 150) & - & 0.75 & 0.30 & 0.23 & 0.33 & 0.822 & 0.878 & 0.849 & 0.23 \\

Llama3-8b & 0.39 & 0.30 & 0.40 & 0.18 & 0.32 & 0.870 & 0.898 & 0.884 & 0.19 \\

Llama3.1-8b & 0.40 & 0.30 & 0.39 & 0.17 & 0.32 & 0.868 & 0.898 & 0.882 & 0.18 \\

Llama3.1-8b-instruct & 0.52 & 0.43 & 0.49 & 0.29 & 0.34 & 0.887 & 0.909 & 0.897 & 0.24 \\ 

Llama3.1-8b (\textbf{code-gen}) & 0.23 & 0.18 & 0.25 & 0.05 & 0.14 & 0.841 & 0.874 & 0.856 & 0.06 \\ 

Llama3.1-8b-instruct (\textbf{code-gen}) & 0.35 & 0.28 & 0.41 & 0.14 & 0.21 & 0.866 & 0.898 & 0.881 & 0.14 \\ 

\bottomrule
\end{tabular}
}
\end{table}


\textbf{Key observations:}
\begin{enumerate}

    \item Performance improvement with instruction tuning: The Llama3.1-8b-instruct model consistently outperformed its non-instructed counterpart across all metrics. For instance, in the code generation task, the instruct model achieved a 52\% higher average permissive accuracy (0.35 vs 0.23) and a 55\% higher average exact accuracy (0.28 vs 0.18) compared to the non-instruct model.
    \item Code generation vs. direct output: Interestingly, for both Llama3.1-8b and Llama3.1-8b-instruct, the direct output outperformed the code generation approach. This suggests that while code generation shows promise, there's still room for improvement in the quality and reliability of the generated code.
    \item Comparison with Langchain: The facebook/opt-1.3b model using Langchain showed impressive performance on its test set, achieving 0.75 exact accuracy. However, it's important to note that this was on a much smaller sample size (150) compared to our main experiments (1000).

\end{enumerate}

To further understand the challenges in the code generation approach, we analyzed the filtered sampling results, presented in Table 5.5.


\begin{table}[h!]
\centering
\caption{Filtered sampling in Code Generation}
\label{table:metrics}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccccccc@{}}
\toprule
Model &  Total samples selected & Remaining samples & Percentage of samples with execution errors & Utilization Rate \\ 
\midrule

%Llama3.1-8b-instruct &  \\ 

Llama3.1-8b (\textbf{code-gen}) &  1000 & 779 & 31.58 & 53.3\\ 

Llama3.1-8b-instruct (\textbf{code-gen}) & 1000 &  843 & 7.12 & 78.3\\ 


\bottomrule
\end{tabular}
}
\end{table}
\textbf{These results reveal crucial insights:}
\begin{enumerate}
   
 \item Sample filtration: A significant number of samples were filtered out due to various issues, primarily code generation failures or execution errors.
 \item Improvement with instruction tuning: The Llama3.1-8b-instruct model showed a marked improvement in code generation reliability. It had fewer samples filtered out (157 vs 221) and a drastically lower percentage of execution errors (7.12\% vs 31.58\%) compared to the non-instruct model.
 \item Utilization rate: The instruct model achieved a much higher utilization rate (78.3\% vs 53.3\%), indicating that it successfully generated and executed code for a larger proportion of the input samples.
\end{enumerate}

\section{Discussion}

Our experiments reveal several important insights into improving number handling capabilities in language models for financial tasks:
\begin{enumerate}
   
 \item \textbf{Model Architecture:} The Llama3-8B architecture demonstrates superior baseline performance compared to Llama2-7B, indicating that newer model iterations have improved inherent number handling abilities.
 \item \textbf{Fine-tuning Effectiveness:} Fine-tuning proves to be a highly effective strategy, significantly boosting performance across all metrics. The fine-tuned Llama3-8B model achieved the best overall results, suggesting that combining advanced architectures with task-specific fine-tuning is a powerful approach.
 \item \textbf{Few-shot Prompting:} While not outperforming fine-tuned models, few-shot prompting strategies show promise, especially when computational resources for full fine-tuning are limited. The similar performance of selective and random few-shot prompting indicates that the quantity of examples may be more crucial than their specific relevance in this domain.
 \item \textbf{Example Selection:} Our results suggest that careful curation of examples for few-shot prompting may not always yield significantly better results than random selection. This finding could simplify the implementation of few-shot learning in practical applications.
 \item \textbf{Tool-based Approaches:} The high performance of the Langchain-based tool on a subset of tasks highlights the potential of combining language models with specialized financial tools for certain applications.
  \item \textbf{Potential of code generation approach:} Despite using models not primarily designed for code generation, we achieved promising results. This suggests that with more specialized models, the performance could be even better.
  \item \textbf{Impact of instruction tuning:} The consistent outperformance of the instruct model demonstrates the value of instruction tuning in improving code generation capabilities and overall task performance.
  \item \textbf{Challenges in code generation:} The high filtration rate and execution errors, especially in the non-instruct model, highlight the challenges in generating reliable, executable code for complex financial calculations.
  \item \textbf{Trade-off between direct output and code generation:} While code generation offers the potential for more precise calculations, our results show that it currently underperforms compared to direct output. This suggests a need for further refinement of the code generation approach.
  \item \textbf{Scalability and resource limitations:} Our experiments were limited to 8b models due to resource constraints. The significant improvement seen with the instruct model suggests that scaling to larger models (e.g., 70b or 405b) could yield even more impressive results, potentially establishing a new state-of-the-art for numerical precision in financial NLP tasks.
\end{enumerate}
% \section{Comparative Analysis}

% \section{Fine-tuning Strategy Optimization}

% \section{Cross-task Generalization}

% \section{Results and Discussion}

\chapter{Conclusions}

%In conclusion, our research demonstrates that significant improvements in number handling capabilities for financial tasks can be achieved through a combination of advanced model architectures, fine-tuning, and strategic use of few-shot prompting. While fine-tuning provides the best overall performance, few-shot prompting offers a computationally lighter alternative with competitive results.

%Future work should explore the integration of language models with specialized financial tools, as well as investigate the scalability of these approaches to larger and more diverse financial datasets. Additionally, further research into the optimal balance between example relevance and quantity in few-shot prompting could yield valuable insights for practical applications in the financial domain.

%Our novel approach of leveraging code generation capabilities in language models shows promise for improving numerical precision in financial tasks. While challenges remain, particularly in the reliability of generated code, the significant improvements seen with instruction tuning and the potential for scaling to larger models suggest this could be a fruitful direction for future research.
%Future work should focus on:

%Experimenting with larger, more specialized models for code generation.
%Refining the few-shot prompting technique to improve code generation reliability.
%Developing more robust error handling and code execution pipelines.
%Extending this approach to other domains requiring high numerical precision.

%By addressing these areas, we believe this approach could potentially set a new standard for numerical accuracy in NLP tasks across various domains.

\section{Limitations}

\section{Future Work}
% \section{Final Reminder}

% The body of your dissertation, before the references and any appendices,
% \emph{must} finish by page~40. The introduction, after preliminary material,
% should have started on page~1.

% You may not change the dissertation format (e.g., reduce the font size, change
% the margins, or reduce the line spacing from the default 1.5 spacing). Be
% careful if you copy-paste packages into your document preamble from elsewhere.
% Some \LaTeX{} packages, such as \texttt{fullpage} or \texttt{savetrees}, change
% the margins of your document. Do not include them!

% Over-length or incorrectly-formatted dissertations will not be accepted and you
% would have to modify your dissertation and resubmit. You cannot assume we will
% check your submission before the final deadline and if it requires resubmission
% after the deadline to conform to the page and style requirements you will be
% subject to the usual late penalties based on your final submission time.

\bibliographystyle{plain}
\bibliography{mybibfile}


% You may delete everything from \appendix up to \end{document} if you don't need it.
\appendix

\chapter{First appendix}

\section{Prompt template}
\label{sec:Prompt template-llama2-7b-chathf}
\begin{tcolorbox}[
    colback=blue!5!white, % Background color
    colframe=white!75!black, % Border color
    width=\textwidth, % Box width
    boxrule=0.5mm, % Border thickness
    sharp corners, % Sharp corners
    title=PROMPT TEMPLATE FOR LLAMA2-7B-CHAT-HF,
    fonttitle=\bfseries,
    coltitle=black,
    toptitle=3mm, % Space above title
    bottomtitle=3mm % Space below title
    ]

\textbf{Prompt\_template:}

\begin{lstlisting}[language={}, basicstyle=\ttfamily\small, frame=single, backgroundcolor=\color{gray!10}]
<s>[INST]
<<SYS>>
{system_prompt}
<</SYS>>
{user_message} [/INST]
\end{lstlisting}

\textbf{system\_prompt:} "You are a helpful AI assistant specializing in financial analysis. Answer the following question based on the given context."

\textbf{user\_message:}
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\small, frame=single, backgroundcolor=\color{gray!10}]
f"Context:\n{context}\n\nQuestion: {question}"
\end{lstlisting}
\end{tcolorbox}


\label{sec:Prompt template-llama3-8b}
\begin{tcolorbox}[
    colback=blue!5!white, % Background color
    colframe=white!75!black, % Border color
    width=\textwidth, % Box width
    boxrule=0.5mm, % Border thickness
    sharp corners, % Sharp corners
    title=PROMPT TEMPLATE FOR LLAMA3-8B,
    fonttitle=\bfseries,
    coltitle=black,
    toptitle=3mm, % Space above title
    bottomtitle=3mm % Space below title
    ]

\textbf{Prompt template:}

%\textbf{Alpaca Prompt:}

\begin{lstlisting}[language={}, basicstyle=\ttfamily\small, frame=single, backgroundcolor=\color{gray!10}]
Below is an instruction that describes a task, 
paired with an input that provides further context. 
Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}
\end{lstlisting}

\end{tcolorbox}

\label{sec:Prompt template-llama3-8b-fsp}
\begin{tcolorbox}[
    colback=blue!5!white, % Background color
    colframe=white!75!black, % Border color
    width=\textwidth, % Box width
    boxrule=0.5mm, % Border thickness
    sharp corners, % Sharp corners
    title=PROMPT TEMPLATE FOR LLAMA3-8B WITH FSP,
    fonttitle=\bfseries,
    coltitle=black,
    toptitle=3mm, % Space above title
    bottomtitle=3mm % Space below title
    ]

\textbf{Prompt template:} \\
%\textbf{Few Shot Prompt:}

\begin{lstlisting}[language={}, basicstyle=\ttfamily\small, frame=single, backgroundcolor=\color{gray!10}, breaklines=true]
You are a financial expert assistant. Answer the following question 
based on the given context. It is CRUCIAL that you use 
step-by-step reasoning and provide detailed numerical calculations where applicable. 
Break down your answer into clear, numbered steps. 
If asked to extract subject and object in relation, return only the single 
most relevant and applicable extraction. Use '\n' for line breaks in your response.

Examples:
{examples}

Now, please answer the following question using the same step-by-step 
approach as demonstrated in the Examples:

Context: {context}
Question: {question}
Answer (including calculation steps and final answer, or single 
most relevant relation extraction, use '\n' for line breaks):"""
\end{lstlisting}

\end{tcolorbox}




\label{sec:Prompt template-code-gen}
\begin{tcolorbox}[
    colback=blue!5!white, % Background color
    colframe=white!75!black, % Border color
    width=\textwidth, % Box width
    boxrule=0.5mm, % Border thickness
    sharp corners, % Sharp corners
    title=IMPROVED FEW SHOT PROMPT TEMPLATE FOR CODE GENERATION AND EXECUTION,
    fonttitle=\bfseries,
    coltitle=black,
    toptitle=3mm, % Space above title
    bottomtitle=3mm % Space below title
    ]

Below is an instruction that describes a task, paired with examples and an input that provides further context. Learn from the examples and write a response that appropriately completes the request.

\textbf{### Instruction:}

You are an AI assistant specialized in financial analysis. Your task is to generate Python code that answers questions based on given financial data and context. Learn from the examples provided and generate accurate, executable Python code that solves the given problem.

\textbf{Important:} When writing Python code, use actual newline characters to separate lines, not '\\n' string literals. Each line of code should be on its own line in your response.

\textbf{### Examples:}

\{examples\}

\textbf{### Input:}

\textbf{Context:}

\{context\}

\textbf{Question:} \{question\}

\textbf{### Response:}

Here's the Python code to answer the question:

\texttt{```python}
\end{tcolorbox}

\section{Examples}

% Below are some sample examples comparing the model’s predictions to the reference values. The best performing model, highlighted in the table earlier, is used to generate these predictions.
\subsection{TAT-QA Dataset}
\begin{tcolorbox}[colback=blue!5!white, % Light blue background
                 colframe=gray, % Gray frame
                 %title=Sample JSON Data, % Title of the box
                 fonttitle=\bfseries, % Bold font for the title
                 breakable, % Allows the box to break across pages
                 sharp corners, % Sharp corners for the box
                 boxsep=10pt, % Padding around text
                 left=10pt, % Left padding
                 right=10pt, % Right padding
                 top=10pt, % Top padding
                 bottom=10pt, % Bottom padding
                 enlarge left by=-10mm, % Extend the box into left margin
                 enlarge right by=-10mm] % Extend the box into right margin
\textbf{Context}: Actuarial assumptions. The Group’s scheme liabilities are measured using the projected unit credit method with principal actuarial assumptions set out below:
\begin{itemize}
    \item Figures represent a weighted average assumption of the individual schemes.
    \item The rate of increases in pensions in payment and deferred revaluation are dependent on the rate of inflation.
\end{itemize}
\textbf{Table}:
\begin{tabular}{lccc}
    & 2019 \% & 2018 \% & 2017 \% \\
    \hline
    Rate of inflation & 2.9 & 2.9 & 3.0 \\
    Rate of increase in salaries & 2.7 & 2.7 & 2.6 \\
    Discount rate & 2.3 & 2.5 & 2.6 \\
\end{tabular}

\textbf{Question}: What does the Weighted average actuarial assumptions consist of? \\
\textbf{Answer}: Rate of inflation, Rate of increase in salaries, Discount rate
\end{tcolorbox}

\subsection{CONVFINQA Dataset}


% Any appendices, including any required ethics information, should be included
% after the references.

% Markers do not have to consider appendices. Make sure that your contributions
% are made clear in the main body of the dissertation (within the page limit).

% \chapter{Participants' information sheet}

% If you had human participants, include key information that they were given in
% an appendix, and point to it from the ethics declaration.

% \chapter{Participants' consent form}

% If you had human participants, include information about how consent was
% gathered in an appendix, and point to it from the ethics declaration.


\end{document}
